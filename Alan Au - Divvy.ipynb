{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge - Divvy dataset (Alan Au)\n",
    "\n",
    "### Data\n",
    "\n",
    "Divvy is a bike-sharing program sponsored by the City of Chicago since 2013.  They have made all of their bike ride data freely available online at https://www.divvybikes.com/system-data.  We’d like to learn about this data — in particular, we are interested in short rides — we’ll define “short” as rides in which the direct distance between the departure and arrival stations is less than 2km.\n",
    "\n",
    "Each trip is anonymized and includes:\n",
    "\n",
    "* Trip start day and time\n",
    "* Trip end day and time\n",
    "* Trip start station\n",
    "* Trip end station\n",
    "* Rider type (Member or 24-Hour Pass User)\n",
    "* If a Member trip, it will also include Member’s gender and year of birth\n",
    "\n",
    "### Task\n",
    "\n",
    "Build a model to predict if a given bike trip will be a short one or not.  You are free to use as much of the Divvy data as you deem appropriate.  But do validate, and don’t overfit.  Moreover, time permitted, please feel free to bring in any publicly available supplementary data that might be useful.  Be careful not to bring in the time machine, i.e., any information that would not be known to Divvy at the start of a trip! Please code your solution in Python.\n",
    "\n",
    " \n",
    "\n",
    "### What Are We Looking For?\n",
    "\n",
    "We are looking for a finished predictive model using whatever tools and libraries that you are comfortable with.  In your submission, please document your methodology (with visualizations where appropriate) and provide ALL the code (including any environment setup and data sources) necessary to reproduce your analysis from scratch. Moreover, please report on the general performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "__author__ = 'Alan Au'\n",
    "__date__  = '2018-12-23'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from string import digits as digits # saves me some typing\n",
    "\n",
    "#Instead of manually typing in file names, I'll just let Python find them for me.\n",
    "data_loc = \"./Divvy/\" #path to my data file directory; end with '/'\n",
    "\n",
    "trip_files = [f for f in listdir(data_loc) if f[6:11] == 'Trips' and f[-4:] == \".csv\"]\n",
    "station_files = [f for f in listdir(data_loc) if f[6:14] == 'Stations' and f[-4:] == \".csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is a haversine distance calculator. I pulled it from https://pypi.python.org/pypi/haversine\n",
    "    \n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "AVG_EARTH_RADIUS = 6371  # in km\n",
    "MILES_PER_KILOMETER = 0.621371\n",
    "\n",
    "def haversine(point1, point2, miles=False):\n",
    "    # unpack latitude/longitude\n",
    "    lat1, lng1 = point1\n",
    "    lat2, lng2 = point2\n",
    "\n",
    "    # convert all latitudes/longitudes from decimal degrees to radians\n",
    "    lat1, lng1, lat2, lng2 = map(radians, (lat1, lng1, lat2, lng2))\n",
    "\n",
    "    # calculate haversine\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = sin(lat * 0.5) ** 2 + cos(lat1) * cos(lat2) * sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * asin(sqrt(d))\n",
    "    if miles:\n",
    "        return h * MILES_PER_KILOMETER # in miles\n",
    "    else:\n",
    "        return h # in kilometers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "The first step is to download and extract the data files of interest. First I did a quick look at the data to see what parts I need. I have both station and trip data.\n",
    "\n",
    "Some notes:\n",
    "* 2013 has a different date format, but that's the only file, so I can just transform it and dump it back out.\n",
    "* Some of the files wrap the data elements in quotation marks, so I will want to strip those off.\n",
    "* Other than those differences, the files seem largely compatible (if large).\n",
    "* Only \"Subscriber\"-type users have gender and birthyear data; I will have to think about that when modeling.\n",
    "\n",
    "### First let's process the stations, so I can get the lat/long and calculate distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "All stations done! Finished in 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Fields are:\n",
    "#id,name,city (2017 only),latitude,longitude,dpcapacity,landmark (2013 only),online date\n",
    "#I'm going to discard the name, city, dpcapacity, landmark, and online date.\n",
    "\n",
    "start = time.mktime(time.localtime())\n",
    "print(\"Running...\") #so I know Jupyter is doing something\n",
    "\n",
    "#for stations\n",
    "def combine_stations(data_loc, stations, out_file):\n",
    "    #inputs: data_loc (directory), stations (list of filenames), out_file (filename)\n",
    "    #outputs: writes to out_file, returns all_stations (dict)\n",
    "    combined_file = open(data_loc+out_file,'w')\n",
    "    all_stations = {}\n",
    "    for station in stations:\n",
    "        raw_file = open(data_loc+station,'r')\n",
    "        raw_data = raw_file.readlines()\n",
    "        year = station[15:19] #pull from filename\n",
    "        if year not in all_stations:\n",
    "            all_stations[year] = {}\n",
    "        for line in raw_data:\n",
    "            line = line.replace('\\\"','') #get rid of quotation marks\n",
    "\n",
    "            if line[0] not in digits: #skip header lines\n",
    "                continue\n",
    "            \n",
    "            if year == '2017': #2017 only\n",
    "                (s_id, s_name, s_city, s_lat, s_long) = line.split(',')[:5]\n",
    "            else:\n",
    "                (s_id, s_name, s_lat, s_long) = line.split(',')[:4]\n",
    "            if s_id not in all_stations[year]: #store stations by year; they sometimes get moved\n",
    "                all_stations[year][s_id] = (float(s_lat),float(s_long))\n",
    "                combined_file.write(','.join([year,s_id,s_lat,s_long])+'\\n')\n",
    "    combined_file.close()\n",
    "    return all_stations #{year:{id:(lat, long)}}\n",
    "\n",
    "stations = combine_stations(data_loc, station_files, \"All_Stations.csv\")\n",
    "\n",
    "duration = time.mktime(time.localtime()) - start\n",
    "print(\"All stations done! Finished in \"+str(duration)+\" seconds.\") #so I know when Jupyter is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's handle trip data.\n",
    "\n",
    "Since I have to look at all of the dates anyways, I might as well decompose the date into features that might be useful for modeling. Specifically, I will probably want the day-of-year, the weekday, and the hour associated with a trip's start and stop. This attempts to account for yearly, weekly, and daily patterns of travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/23/2017 9:30 ('357', '5', '9')\n"
     ]
    }
   ],
   "source": [
    "#helper functions\n",
    "def new_2013_date(old_date):\n",
    "    #old format is yyyy-mm-dd hh:mm\n",
    "    #new format is mm/dd/yyyy hh:mm\n",
    "    parts = old_date.split()\n",
    "    (y,m,d) = parts[0].split('-')\n",
    "    new_date = '/'.join([m,d,y])+\" \"+parts[1]\n",
    "    return new_date\n",
    "\n",
    "def process_date(date):\n",
    "    #input: mm/dd/yyyy hh:mm\n",
    "    parts = date.split()\n",
    "    (m,d,y) = list(map(int,parts[0].split('/')))\n",
    "    day_of_year = str(datetime.datetime(y, m, d).timetuple().tm_yday)\n",
    "    weekday = str(datetime.datetime(y, m, d).weekday()) #Monday = 0, etc.\n",
    "    hour = parts[1].split(':')[0] #get the first part of hh:mm\n",
    "    return (day_of_year, weekday, hour)\n",
    "\n",
    "testdate = \"2017-12-23 9:30\" #Saturday\n",
    "testdate = new_2013_date(testdate)\n",
    "print(testdate, process_date(testdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fields are:\n",
    "#trip_id,start,stop,bike,duration,from_id,from_name,to_id,to_name,usertype,gender,birthyear\n",
    "\n",
    "#for trips\n",
    "def combine_trips(data_loc, trips, stations, out_file):\n",
    "    #inputs: data_loc (directory), trips (list of filenames), out_file (filename)\n",
    "    #outputs: writes to out_file, returns none\n",
    "    combined_file = open(data_loc+out_file,'w')\n",
    "    \n",
    "    header = \"year,start_day,start_weekday,start_hour,stop_day,stop_weekday,stop_hour,bike,duration,from_id,to_id,distance,short,usertype,gender,birthyear\\n\"\n",
    "    combined_file.write(header)\n",
    "    for trip in trips:\n",
    "        year = trip[12:16] #from filename\n",
    "        raw_file = open(data_loc+trip,'r')\n",
    "        raw_data = raw_file.readlines()\n",
    "        for line in raw_data:\n",
    "            line = line.strip().replace('\\\"','') #get rid of quotation marks\n",
    "\n",
    "            if line[0] not in digits: #skip header lines\n",
    "                continue\n",
    "\n",
    "            (t_id,t_start,t_stop,t_bike,t_dur,t_from_id,t_from_name,t_to_id,t_to_name,t_usertype,t_gender,t_birth) = line.split(',')\n",
    "            \n",
    "            if year == '2013': #process 2013 dates separately\n",
    "                t_start = new_2013_date(t_start)\n",
    "                t_stop = new_2013_date(t_stop)\n",
    "\n",
    "            t_start_doy,t_start_weekday,t_start_hour = process_date(t_start)\n",
    "            t_stop_doy,t_stop_weekday,t_stop_hour = process_date(t_stop)\n",
    "                        \n",
    "            t_dist = '0'\n",
    "            try:\n",
    "                t_dist = str(haversine(stations[year][t_from_id],stations[year][t_to_id])) #calculate distance between stations in km\n",
    "            except KeyError:\n",
    "                t_dist = str(haversine(stations['2017'][t_from_id],stations['2017'][t_to_id])) #if not found, try most recent station list\n",
    "                #t_dist = str(float(t_dur)/360) #estimated distance based on 10km/h speed\n",
    "\n",
    "            t_short = '0'\n",
    "            if float(t_dist) < 2.0: \n",
    "                t_short = '1'\n",
    "            \n",
    "            #I'm throwing out trip_id (probably not useful) and station names (redundant w/station ID)\n",
    "            out_line = ','.join([year, t_start_doy,t_start_weekday,t_start_hour,\n",
    "                                 t_stop_doy,t_stop_weekday,t_stop_hour,\n",
    "                                 t_bike,t_dur,t_from_id,t_to_id,t_dist,t_short,t_usertype,t_gender,t_birth])+\"\\n\"\n",
    "            combined_file.write(out_line)\n",
    "    combined_file.close() #clean up after ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "All trips done! Finished in 597.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.mktime(time.localtime())\n",
    "print(\"Running...\") #so I know Jupyter is doing something\n",
    "\n",
    "combine_trips(data_loc, trip_files, stations, \"All_Trips.csv\")\n",
    "\n",
    "duration = time.mktime(time.localtime()) - start\n",
    "print(\"All trips done! Finished in \"+str(duration)+\" seconds.\") #so I know when Jupyter is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's a lot of data, like 884 Mb of it, so I'm going to generate subsets by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Trips-by-year done! Finished in 600.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.mktime(time.localtime())\n",
    "print(\"Running...\") #so I know Jupyter is doing something\n",
    "\n",
    "all_trips = {}\n",
    "for trip_file in trip_files:\n",
    "    year = trip_file[12:16] #get the year number\n",
    "    if year in all_trips:\n",
    "        all_trips[year].append(trip_file)\n",
    "    else:\n",
    "        all_trips[year] = [trip_file]\n",
    "\n",
    "for trip_year in all_trips: #for each year, process its sublist of associated files\n",
    "    out_name = \"Trips_\"+trip_year+\".csv\"\n",
    "    combine_trips(data_loc, all_trips[trip_year], stations, out_name)\n",
    "\n",
    "duration = time.mktime(time.localtime()) - start\n",
    "print(\"Trips-by-year done! Finished in \"+str(duration)+\" seconds.\") #so I know when Jupyter is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some thoughts\n",
    "\n",
    "Note that there's a quirk where some rides start and stop at the same station. This could be a round-trip, but I can't readily tell. A quick Google search suggest that the average in-city bike speed is about 10-15 km/h. \n",
    "\n",
    "The original problem statement only asks for staight-line distances calculated between stations, but if needed, I could conservatively estimate the distance for round-trips based on duration: ```distance(km) = duration(sec) * 10(km/h) * 1/3600(h/sec)``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion\n",
    "\n",
    "Now I have data files that are all in a standardized format, with distance between stations and a set of features. \n",
    "\n",
    "1. year (2013-2017)\n",
    "2. start day-of-year (1-366)\n",
    "3. start weekday (0-6, starts on Monday)\n",
    "5. start hour (0-23)\n",
    "6. stop day-of-year (1-366)\n",
    "7. stop weekday (0-6, starts on Monday)\n",
    "8. stop hour (0-23)\n",
    "9. bike id (integer)\n",
    "10. trip duration (seconds)\n",
    "11. from station (integer)\n",
    "12. to station (integer)\n",
    "13. trip distance (km)\n",
    "14. short trip ('1' if distance < 2.0 km)\n",
    "15. user type ('Customer' or 'Subscriber')\n",
    "16. gender (if 'Subscriber')\n",
    "17. birth year (if 'Subscriber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
