{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge - Divvy dataset (Alan Au)\n",
    "\n",
    "### Data\n",
    "\n",
    "Divvy is a bike-sharing program sponsored by the City of Chicago since 2013.  They have made all of their bike ride data freely available online at https://www.divvybikes.com/system-data.  We’d like to learn about this data — in particular, we are interested in short rides — we’ll define “short” as rides in which the direct distance between the departure and arrival stations is less than 2km.\n",
    "\n",
    "Each trip is anonymized and includes:\n",
    "\n",
    "* Trip start day and time\n",
    "* Trip end day and time\n",
    "* Trip start station\n",
    "* Trip end station\n",
    "* Rider type (Member or 24-Hour Pass User)\n",
    "* If a Member trip, it will also include Member’s gender and year of birth\n",
    "\n",
    "### Task\n",
    "\n",
    "Build a model to predict if a given bike trip will be a short one or not.  You are free to use as much of the Divvy data as you deem appropriate.  But do validate, and don’t overfit.  Moreover, time permitted, please feel free to bring in any publicly available supplementary data that might be useful.  Be careful not to bring in the time machine, i.e., any information that would not be known to Divvy at the start of a trip! Please code your solution in Python.\n",
    "\n",
    " \n",
    "\n",
    "### What Are We Looking For?\n",
    "\n",
    "We are looking for a finished predictive model using whatever tools and libraries that you are comfortable with.  In your submission, please document your methodology (with visualizations where appropriate) and provide ALL the code (including any environment setup and data sources) necessary to reproduce your analysis from scratch. Moreover, please report on the general performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "__author__ = 'Alan Au'\n",
    "__date__  = '2018-12-22'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "from os import listdir\n",
    "from string import digits as digits # saves me some typing\n",
    "\n",
    "#Instead of manually typing in file names, I'll just let Python find them for me.\n",
    "data_loc = \"./Divvy/\" #path to my data file directory; end with '/'\n",
    "\n",
    "trip_files = [f for f in listdir(data_loc) if f[6:11] == 'Trips' and f[-4:] == \".csv\"]\n",
    "station_files = [f for f in listdir(data_loc) if f[6:14] == 'Stations' and f[-4:] == \".csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a haversine distance calculator. I pulled it from https://pypi.python.org/pypi/haversine\n",
    "    \n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "AVG_EARTH_RADIUS = 6371  # in km\n",
    "MILES_PER_KILOMETER = 0.621371\n",
    "\n",
    "def haversine(point1, point2, miles=False):\n",
    "    # unpack latitude/longitude\n",
    "    lat1, lng1 = point1\n",
    "    lat2, lng2 = point2\n",
    "\n",
    "    # convert all latitudes/longitudes from decimal degrees to radians\n",
    "    lat1, lng1, lat2, lng2 = map(radians, (lat1, lng1, lat2, lng2))\n",
    "\n",
    "    # calculate haversine\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = sin(lat * 0.5) ** 2 + cos(lat1) * cos(lat2) * sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * asin(sqrt(d))\n",
    "    if miles:\n",
    "        return h * MILES_PER_KILOMETER # in miles\n",
    "    else:\n",
    "        return h # in kilometers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "The first step is to download and extract the data files of interest. First I did a quick look at the data to see what parts I need. I have both station and trip data.\n",
    "\n",
    "Some notes:\n",
    "* 2013 has a different date format, but that's the only file, so I can just transform it and dump it back out.\n",
    "* Some of the files wrap the data elements in quotation marks, so I will want to strip those off.\n",
    "* Other than those differences, the files seem largely compatible (if large).\n",
    "* Only \"Subscriber\"-type users have gender and birthyear data; I will have to think about that when modeling.\n",
    "\n",
    "### First let's process the stations, so I can get the lat/long and calculate distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "All stations done! Finished in 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Fields are:\n",
    "#id,name,city (2017 only), latitude,longitude,dpcapacity,landmark (2013 only),online date\n",
    "#I'm going to discard the name, city, dpcapacity, landmark, and online date.\n",
    "\n",
    "start = time.mktime(time.localtime())\n",
    "print(\"Running...\") #so I know Jupyter is doing something\n",
    "\n",
    "#for stations\n",
    "def combine_stations(data_loc, stations, out_file):\n",
    "    #inputs: data_loc (directory), stations (list of filenames), out_file (filename)\n",
    "    #outputs: writes to out_file, returns all_stations (dict)\n",
    "    combined_file = open(data_loc+out_file,'w')\n",
    "    all_stations = {}\n",
    "    for station in stations:\n",
    "        raw_file = open(data_loc+station,'r')\n",
    "        raw_data = raw_file.readlines()\n",
    "        for line in raw_data:\n",
    "            line = line.replace('\\\"','') #get rid of quotation marks\n",
    "\n",
    "            if line[0] not in digits: #skip header lines\n",
    "                continue\n",
    "            \n",
    "            if station[15:19] == '2017': #2017 only\n",
    "                (s_id, s_name, s_city, s_lat, s_lon) = line.split(',')[:5]\n",
    "            else:\n",
    "                (s_id, s_name, s_lat, s_lon) = line.split(',')[:4]\n",
    "            if s_id not in all_stations: #de-duplicate; note it only logs a station the first time it sees it\n",
    "                all_stations[s_id] = (float(s_lat),float(s_lon))\n",
    "                combined_file.write(','.join([s_id,s_lat,s_lon])+'\\n')\n",
    "    combined_file.close()\n",
    "    return all_stations #{id:(lat, lon)}\n",
    "\n",
    "stations = combine_stations(data_loc, station_files, \"All_Stations.csv\")\n",
    "\n",
    "duration = time.mktime(time.localtime()) - start\n",
    "print(\"All stations done! Finished in \"+str(duration)+\" seconds.\") #so I know when Jupyter is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's handle trip data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fields are:\n",
    "#trip_id,start,stop,bike,duration,from_id,from_name,to_id,to_name,usertype,gender,birthyear\n",
    "\n",
    "#helper function to process 2013 dates\n",
    "def new_2013_date(old_date):\n",
    "    #old format is yyyy-mm-dd hh:mm\n",
    "    #new format is mm/dd/yyyy hh:mm\n",
    "    parts = old_date.split()\n",
    "    (y,m,d) = parts[0].split('-')\n",
    "    new_date = '/'.join(m,d,y)+\" \"+parts[1]\n",
    "    return new_date\n",
    "\n",
    "#for trips\n",
    "def combine_trips(data_loc, trips, stations, out_file):\n",
    "    #inputs: data_loc (directory), trips (list of filenames), out_file (filename)\n",
    "    #outputs: writes to out_file, returns none\n",
    "    combined_file = open(data_loc+out_file,'w')\n",
    "    for trip in trips:\n",
    "        raw_file = open(data_loc+trip,'r')\n",
    "        raw_data = raw_file.readlines()\n",
    "        for line in raw_data:\n",
    "            line = line.strip().replace('\\\"','') #get rid of quotation marks\n",
    "\n",
    "            if line[0] not in digits: #skip header lines\n",
    "                continue\n",
    "\n",
    "            (t_id,t_start,t_stop,t_bike,t_dur,t_from_id,t_from_name,t_to_id,t_to_name,t_usertype,t_gender,t_birth) = line.split(',')\n",
    "            \n",
    "            if csv == 'Divvy_Trips_2013.csv': #process 2013 dates separately\n",
    "                t_start = new_2013_date(t_start)\n",
    "                t_stop = new_2013_date(t_stop)\n",
    "\n",
    "            t_dist = str(haversine(stations[t_from_id],stations[t_to_id])) #calculate distance between stations in km\n",
    "            \n",
    "            #I'm throwing out trip_id and bike_id (probably not useful), and station names (redundant w/station ID)\n",
    "            out_line = ','.join([t_start,t_stop,t_dur,t_from_id,t_to_id,t_dist,t_usertype,t_gender,t_birth])+\"\\n\"\n",
    "            combined_file.write(out_line)\n",
    "    combined_file.close() #clean up after ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "All trips done! Finished in 216.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.mktime(time.localtime())\n",
    "print(\"Running...\") #so I know Jupyter is doing something\n",
    "\n",
    "combine_trips(data_loc, trip_files, stations, \"All_Trips.csv\")\n",
    "\n",
    "duration = time.mktime(time.localtime()) - start\n",
    "print(\"All trips done! Finished in \"+str(duration)+\" seconds.\") #so I know when Jupyter is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's a lot of data, like 932 Mb of it, so I'm going to generate subsets by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Trips-by-year done! Finished in 198.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.mktime(time.localtime())\n",
    "print(\"Running...\") #so I know Jupyter is doing something\n",
    "\n",
    "all_trips = {}\n",
    "for trip_file in trip_files:\n",
    "    year = trip_file[12:16] #get the year number\n",
    "    if year in all_trips:\n",
    "        all_trips[year].append(trip_file)\n",
    "    else:\n",
    "        all_trips[year] = [trip_file]\n",
    "\n",
    "for trip_year in all_trips: #for each year, process its sublist of associated files\n",
    "    out_name = \"Trips_\"+trip_year+\".csv\"\n",
    "    combine_trips(data_loc, all_trips[trip_year], stations, out_name)\n",
    "\n",
    "duration = time.mktime(time.localtime()) - start\n",
    "print(\"Trips-by-year done! Finished in \"+str(duration)+\" seconds.\") #so I know when Jupyter is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some thoughts\n",
    "\n",
    "So far, things look good, but there's a quirk where some rides start and stop at the same station. This could be a round-trip, but I can't readily tell. a quick Google search suggest that the average in-city bike speed is about 15 km/h, but of course, there's no guarantee that someone is using the bike continuously between check-ins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion\n",
    "\n",
    "Now I have data files that are all in a standardized format, with distance between stations and a set of features. I probably also want to pull out some date features, like day-of-week and hour-of-start.\n",
    "\n",
    "I'll want to come up with a proxy distance when it's listed as 0: ```distance(km) = duration(sec) * 15(km/h) * 1/3600(h/sec)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
